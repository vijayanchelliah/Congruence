import asyncio
import time
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks, Depends, Response
from fastapi.responses import FileResponse, JSONResponse, RedirectResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import torch
import yaml
import os
import tempfile
import shutil
import re
from urlextract import URLExtract
from docx import Document
import datetime
from pdf2docx import parse
import uvicorn
import logging
import pathlib
from typing import List, Dict, Any, Optional
from azure.storage.blob import BlobServiceClient
from azure.core.exceptions import ResourceExistsError, ResourceNotFoundError
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor
import aiofiles
import aiohttp
from functools import partial

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Define FastAPI app
app = FastAPI(
    title="Async Translation API"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Define Request Models


class TranslationRequest(BaseModel):
    text: str
    src_lang: str
    tgt_lang: str


class DocumentTranslationRequest(BaseModel):
    src_lang: str
    tgt_langs: List[str]


class TranslationResponse(BaseModel):
    translated_text: str


class DownloadLink(BaseModel):
    language: str
    filename: str
    download_url: str


class DocumentTranslationResponse(BaseModel):
    message: str
    translated_files: List[DownloadLink]


# Global variables for model and tokenizer
global_model = None
global_tokenizer = None
global_device = None
translation_semaphore = None
MAX_CONCURRENT_TRANSLATIONS = 5  # Adjust based on your system's capabilities

# Language mapping dictionary (same as in api.py)
language_mapping = {
    "English": "eng_Latn",
    "Bulgarian": "bul_Cyrl",
    "Chinese (Simplified)": "zho_Hans",
    "Chinese (Traditional)": "zho_Hant",
    "Croatian": "hrv_Latn",
    "Czech": "ces_Latn",
    "Danish": "dan_Latn",
    "Dutch": "nld_Latn",
    "Estonian": "est_Latn",
    "Finnish": "fin_Latn",
    "French": "fra_Latn",
    "German": "deu_Latn",
    "Greek": "ell_Grek",
    "Hungarian": "hun_Latn",
    "Icelandic": "isl_Latn",
    "Indonesian": "ind_Latn",
    "Italian": "ita_Latn",
    "Kazakh": "kaz_Cyrl",
    "Korean": "kor_Hang",
    "Latvian": "lvs_Latn",
    "Lithuanian": "lit_Latn",
    "Macedonian": "mkd_Cyrl",
    "Norwegian Nynorsk": "nno_Latn",
    "Norwegian Bokmål": "nob_Latn",
    "Polish": "pol_Latn",
    "Portuguese": "por_Latn",
    "Romanian": "ron_Latn",
    "Russian": "rus_Cyrl",
    "Serbian": "srp_Cyrl",
    "Slovak": "slk_Latn",
    "Slovakian": "slk_Latn",
    "Slovenian": "slv_Latn",
    "Spanish": "spa_Latn",
    "Swedish": "swe_Latn",
    "Turkish": "tur_Latn",
    "Vietnamese": "vie_Latn",
}

# Add reverse mapping for convenience
reverse_language_mapping = {
    code: name for name, code in language_mapping.items()}


def detect_environment():
    """Determine the best available device for PyTorch."""
    if torch.cuda.is_available():
        logger.info("Using CUDA for translation")
        return 'cuda'
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        logger.info("Using MPS (Metal Performance Shaders) for translation")
        return 'mps'
    logger.info("Using CPU for translation")
    return 'cpu'


# Setup Paths & Config
BASE_DIR = pathlib.Path(__file__).parent.resolve()
MODEL_DIR = os.environ.get("MODEL_DIR", str(BASE_DIR / "statics" / "Models"))
weights_path = os.path.join(MODEL_DIR, "best_model_new.pth")
config_path = os.path.join(MODEL_DIR, "training_config.yaml")
cache_path = os.path.join(MODEL_DIR, "cache")

# Ensure directories exist
pathlib.Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)
pathlib.Path(cache_path).mkdir(parents=True, exist_ok=True)
pathlib.Path(BASE_DIR / "translated").mkdir(parents=True, exist_ok=True)


class TranslationCache:
    """Thread-safe cache for storing translation results."""

    def __init__(self, max_size=1000):
        self.cache = {}
        self.max_size = max_size
        self._lock = asyncio.Lock()

    async def get(self, key):
        """Get a cached translation if available."""
        async with self._lock:
            return self.cache.get(key)

    async def set(self, key, value):
        """Set a new translation in the cache."""
        async with self._lock:
            if len(self.cache) >= self.max_size:
                self.cache.pop(next(iter(self.cache)))
            self.cache[key] = value


# Initialize cache
translation_cache = TranslationCache()


async def translate_text_async(text, src_lang, tgt_lang):
    """Asynchronous translation function with caching."""
    # Check cache first
    cache_key = f"{text}|{src_lang}|{tgt_lang}"
    cached_result = await translation_cache.get(cache_key)
    if cached_result:
        return cached_result

    # Use semaphore to limit concurrent translations
    async with translation_semaphore:
        try:
            # Run translation in a thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                # Create a new pipeline instance for each translation
                translator = await loop.run_in_executor(
                    pool,
                    lambda: pipeline(
                        "translation",
                        model=global_model,
                        tokenizer=global_tokenizer,
                        src_lang=src_lang,
                        tgt_lang=tgt_lang,
                        max_length=400,
                        device=global_device
                    )
                )

                # Perform translation
                result = await loop.run_in_executor(
                    pool,
                    lambda: translator(text)
                )

                translated_text = result[0]['translation_text']

                # Store in cache
                await translation_cache.set(cache_key, translated_text)

                return translated_text
        except Exception as e:
            logger.error(f"Translation error: {str(e)}")
            # return f"[ERROR TRANSLATING: {str(e)}] {text}"


async def translate_text_by_sentences_async(text, src_lang, tgt_lang):
    """Translate text sentence by sentence asynchronously."""
    # Initialize placeholder system
    placeholder_system = PlaceholderSystem()
    placeholder_system.add_type('trademark', 'thtm')
    placeholder_system.add_type('url', 'URL')

    # Preprocess text
    text = re.sub(r'\b(\w+)TM', r'\1™', text)
    text_with_placeholders = placeholder_system.apply_placeholders(text)

    # Split text into sentences
    sentence_pattern = r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|!)\s'
    sentences = re.split(sentence_pattern, text_with_placeholders)

    # Process sentences in batches to avoid overwhelming the system
    batch_size = 5  # Adjust based on your system's capabilities
    translated_sentences = []

    for i in range(0, len(sentences), batch_size):
        batch = sentences[i:i + batch_size]
        batch_tasks = []
        for sentence in batch:
            if sentence.strip():
                batch_tasks.append(
                    translate_text_async(sentence, src_lang, tgt_lang)
                )

        # Wait for current batch to complete
        batch_results = await asyncio.gather(*batch_tasks)
        translated_sentences.extend(batch_results)

    # Join translated sentences
    translated_text = " ".join(translated_sentences)

    # Restore placeholders
    final_translated_text = placeholder_system.restore_placeholders(
        translated_text)

    return final_translated_text


class PlaceholderSystem:
    """Improved placeholder system for handling special content during translation."""

    def __init__(self):
        self.types = {}
        self.instances = {}
        self.counters = {}

    def add_type(self, type_key, prefix):
        """Register a new placeholder type"""
        self.types[type_key] = prefix
        self.instances[type_key] = []
        self.counters[type_key] = 0
        return self

    def create_placeholder(self, type_key, original_content):
        """Create a placeholder for specific content"""
        if type_key not in self.types:
            raise ValueError(f"Unknown placeholder type: {type_key}")

        placeholder = f"[{self.types[type_key]}{self.counters[type_key]}]"
        self.instances[type_key].append((placeholder, original_content))
        self.counters[type_key] += 1
        return placeholder

    def apply_placeholders(self, text):
        """Apply placeholders to text"""
        modified_text = text

        # Process trademark symbols
        if 'trademark' in self.types:
            trademark_pattern = r'(\w+\s?™)'
            trademarked_words = re.findall(trademark_pattern, modified_text)
            for word in trademarked_words:
                placeholder = self.create_placeholder('trademark', word)
                modified_text = modified_text.replace(word, placeholder)

        # Process URLs
        if 'url' in self.types:
            extractor = URLExtract()
            urls = extractor.find_urls(modified_text)
            for url in urls:
                url_pos = modified_text.find(url)
                if url_pos >= 0:
                    space_before = modified_text[url_pos -
                                                 1:url_pos] if url_pos > 0 else ""
                    space_after = modified_text[url_pos+len(url):url_pos+len(
                        url)+1] if url_pos+len(url) < len(modified_text) else ""

                    url_with_context = {
                        'text': url,
                        'space_before': space_before.isspace(),
                        'space_after': space_after.isspace()
                    }

                    placeholder = self.create_placeholder(
                        'url', url_with_context)
                    modified_text = modified_text.replace(url, placeholder)

        return modified_text

    def restore_placeholders(self, text):
        """Restore placeholders in text"""
        modified_text = text
        modified_text = re.sub(
            r'\[\s*([a-zA-Z]+)\s*(\d+)\s*\]', r'[\1\2]', modified_text)

        restored_placeholders = {type_key: set() for type_key in self.types}

        for type_key, instances in self.instances.items():
            for placeholder, original in instances:
                clean_placeholder = placeholder.strip('[]')
                pattern = rf'\[\s*{clean_placeholder}\s*\]'

                if re.search(pattern, modified_text):
                    if isinstance(original, dict) and 'text' in original:
                        replacement = original['text']

                        if type_key == 'url':
                            prefix = " " if original.get(
                                'space_before', False) else ""
                            suffix = " " if original.get(
                                'space_after', False) else ""
                            replacement = f"{prefix}{replacement}{suffix}"

                        modified_text = re.sub(
                            pattern, replacement, modified_text)
                    else:
                        modified_text = re.sub(
                            pattern, original, modified_text)

                    restored_placeholders[type_key].add(placeholder)

        modified_text = re.sub(r'\s{2,}', ' ', modified_text)
        return modified_text


@app.on_event("startup")
async def startup_event():
    """Initialize the model and other resources on startup."""
    global global_model, global_tokenizer, global_device, translation_semaphore

    try:
        # Initialize semaphore for concurrent translations
        translation_semaphore = asyncio.Semaphore(MAX_CONCURRENT_TRANSLATIONS)

        # Set device
        global_device = detect_environment()

        # Load config
        if not os.path.exists(config_path):
            logger.warning(
                f"Config file not found at {config_path}. Using default model.")
            config = {"model": {"name": "facebook/nllb-200-distilled-600M"}}
        else:
            with open(config_path, "r") as file:
                config = yaml.safe_load(file)
                if not config:
                    logger.warning(
                        "Config file is empty. Using default model.")
                    config = {
                        "model": {"name": "facebook/nllb-200-distilled-600M"}}

        MODEL_NAME = config.get("model", {}).get(
            "name", "facebook/nllb-200-distilled-600M")

        logger.info(f"Loading tokenizer and model {MODEL_NAME}...")

        # Load model and tokenizer
        global_tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            cache_dir=cache_path
        )
        global_model = AutoModelForSeq2SeqLM.from_pretrained(
            MODEL_NAME,
            cache_dir=cache_path
        )

        # Load custom weights if available
        if os.path.exists(weights_path):
            logger.info(f"Loading custom weights from {weights_path}")
            state_dict = torch.load(weights_path, map_location=global_device)
            global_model.load_state_dict(state_dict, strict=False)

        global_model.to(global_device)
        logger.info(
            "Model loaded successfully and moved to device: %s", global_device)

    except Exception as e:
        logger.error(f"Error during startup: {str(e)}")
        raise RuntimeError(f"Failed to initialize: {str(e)}")


@app.get("/health")
async def health_check():
    """Check if the service is running properly."""
    return {"status": "ok", "timestamp": datetime.datetime.now().isoformat()}


@app.get("/languages")
async def get_languages():
    """Get the list of supported languages."""
    return {"languages": list(language_mapping.keys())}


@app.post("/translate-text", response_model=TranslationResponse)
async def translate(request: TranslationRequest):
    """Translate text from source language to target language."""
    try:
        # Map language codes
        src_lang_code = language_mapping.get(
            request.src_lang, request.src_lang)
        tgt_lang_code = language_mapping.get(
            request.tgt_lang, request.tgt_lang)

        # Validate language codes
        if src_lang_code not in language_mapping.values():
            logger.warning(f"Unknown source language: {request.src_lang}")
        if tgt_lang_code not in language_mapping.values():
            logger.warning(f"Unknown target language: {request.tgt_lang}")

        translated_text = await translate_text_by_sentences_async(
            request.text,
            src_lang_code,
            tgt_lang_code
        )

        logger.info(
            f"Translated text from {request.src_lang} to {request.tgt_lang}")
        return {"translated_text": translated_text}
    except Exception as e:
        logger.error(f"Error in text translation: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


def preserve_run_formatting(original_run, new_text):
    """Preserve the formatting of a run while updating its text."""
    # Capture original formatting
    try:
        font = original_run.font
        original_format = {
            'bold': original_run.bold,
            'italic': original_run.italic,
            'underline': original_run.underline,
            'strike': font.strike if hasattr(font, 'strike') else None,
            'size': font.size if hasattr(font, 'size') else None,
            'name': font.name if hasattr(font, 'name') else None,
            'color': font.color.rgb if font.color and hasattr(font.color, 'rgb') else None,
            'highlight_color': font.highlight_color if hasattr(font, 'highlight_color') else None
        }

        # Add more format properties if they exist
        for prop in ['all_caps', 'small_caps', 'subscript', 'superscript',
                     'shadow', 'emboss', 'imprint', 'outline']:
            if hasattr(font, prop):
                original_format[prop] = getattr(font, prop)

        # Update text
        original_run.text = new_text

        # Reapply basic formatting
        original_run.bold = original_format['bold']
        original_run.italic = original_format['italic']
        original_run.underline = original_format['underline']

        # Reapply font attributes
        for prop, value in original_format.items():
            if prop not in ['bold', 'italic', 'underline'] and value is not None:
                try:
                    if prop == 'color':
                        if hasattr(font, 'color') and font.color and hasattr(font.color, 'rgb'):
                            font.color.rgb = value
                    elif prop == 'highlight_color':
                        if hasattr(font, 'highlight_color'):
                            font.highlight_color = value
                    elif hasattr(font, prop):
                        setattr(font, prop, value)
                except Exception as e:
                    logger.debug(f"Could not set font property {prop}: {e}")
    except Exception as e:
        logger.warning(f"Error preserving run formatting: {e}")
        # Fallback - just set the text without formatting
        original_run.text = new_text


def preserve_capitalization(original_text, translated_text):
    """Preserve the capitalization pattern from original text in the translated text."""
    # If original is empty or translated is empty, return translated as is
    if not original_text or not translated_text:
        return translated_text

    try:
        if original_text.isupper():
            return translated_text.upper()
        if original_text.istitle():
            return translated_text.title()
        if len(original_text) > 0 and original_text[0].isupper() and not original_text[1:].isupper():
            # Capitalize only the first character of the translated text
            if translated_text:
                return translated_text[0].upper() + translated_text[1:]
    except Exception as e:
        logger.warning(f"Error preserving capitalization: {e}")

    # Default: return translated text as is
    return translated_text


async def translate_paragraph_with_formatting_async(paragraph, src_lang, tgt_lang):
    """Translate paragraph while preserving formatting asynchronously."""
    try:
        # Store paragraph properties
        para_properties = {}
        for prop in ['alignment', 'style']:
            if hasattr(paragraph, prop):
                para_properties[prop] = getattr(paragraph, prop)

        # Store paragraph format properties
        para_format_props = {}
        if hasattr(paragraph, 'paragraph_format'):
            for prop in ['line_spacing', 'space_before', 'space_after', 'first_line_indent']:
                if hasattr(paragraph.paragraph_format, prop):
                    para_format_props[prop] = getattr(
                        paragraph.paragraph_format, prop)

        # Get all runs and their text
        runs_with_text = [(run, run.text)
                          for run in paragraph.runs if run.text.strip()]

        if not runs_with_text:
            return

        # Process runs in batches
        batch_size = 3  # Adjust based on your system's capabilities
        for i in range(0, len(runs_with_text), batch_size):
            batch = runs_with_text[i:i + batch_size]
            batch_tasks = []

            for run, text in batch:
                # Skip empty runs or special formatting runs
                if not text.strip() or (hasattr(run, 'font') and hasattr(run.font, 'size') and run.font.size == 279400):
                    continue

                batch_tasks.append(
                    translate_text_by_sentences_async(text, src_lang, tgt_lang)
                )

            # Wait for current batch to complete
            translated_texts = await asyncio.gather(*batch_tasks)

            # Update runs with translated text
            for (run, _), translated_text in zip(batch, translated_texts):
                try:
                    # Preserve capitalization patterns from original text
                    translated_text = preserve_capitalization(
                        run.text, translated_text)

                    # Update run text while preserving formatting
                    preserve_run_formatting(run, translated_text.strip())
                except Exception as e:
                    logger.error(f"Error updating run text: {e}")
                    continue

        # Restore paragraph formatting
        try:
            for prop, value in para_properties.items():
                setattr(paragraph, prop, value)

            for prop, value in para_format_props.items():
                if hasattr(paragraph.paragraph_format, prop):
                    setattr(paragraph.paragraph_format, prop, value)
        except Exception as e:
            logger.warning(f"Error restoring paragraph formatting: {e}")
    except Exception as e:
        logger.error(
            f"Error in translate_paragraph_with_formatting_async: {e}")


# Convert PDF to DOCX
def convert_pdf_to_docx(pdf_path, docx_path):
    """Convert PDF to DOCX format."""
    try:
        # Check if the PDF exists
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"PDF file not found: {pdf_path}")

        parse(pdf_path, docx_path, start=0, end=None)
        logger.info("PDF to DOCX conversion completed.")

        # Verify the DOCX was created
        if not os.path.exists(docx_path):
            raise Exception(f"DOCX file was not created at {docx_path}")

        return True
    except Exception as e:
        logger.error(f"PDF to DOCX conversion failed: {e}")
        raise Exception(f"PDF to DOCX conversion failed: {e}")


async def translate_table_async(table, src_lang, tgt_lang):
    """Translate all text in a table asynchronously, including nested tables."""
    try:
        for row in table.rows:
            for cell in row.cells:
                # First, translate the direct text inside the cell
                for paragraph in cell.paragraphs:
                    if paragraph.text.strip():
                        await translate_paragraph_with_formatting_async(
                            paragraph,
                            src_lang,
                            tgt_lang,
                        )

                # If the cell contains a nested table, process it recursively
                if hasattr(cell, 'tables') and cell.tables:
                    for nested_table in cell.tables:
                        await translate_table_async(nested_table, src_lang, tgt_lang)
    except Exception as e:
        logger.error(f"Error translating table: {e}")


async def translate_docx_async(doc_path, src_lang, tgt_langs, temp_dir, input_file_name):
    """Translate a DOCX document to multiple target languages asynchronously."""
    translated_files = {}

    # Create log directory
    log_dir = os.path.join("logs")
    os.makedirs(log_dir, exist_ok=True)

    # Translation statistics
    stats = {
        "total_paragraphs": 0,
        "total_tables": 0,
        "languages": {}
    }

    async def process_language(tgt_lang):
        try:
            # Open the document
            doc = Document(doc_path)

            # Capture statistics
            stats["total_paragraphs"] = len(doc.paragraphs)
            stats["total_tables"] = len(doc.tables)
            start_time = datetime.datetime.now()
            last_print_time = start_time
            stats["languages"][tgt_lang] = {"start_time": start_time}

            # Translate paragraphs concurrently
            paragraph_tasks = []
            for para in doc.paragraphs:
                if para.text.strip():
                    paragraph_tasks.append(
                        translate_paragraph_with_formatting_async(
                            para, src_lang, tgt_lang)
                    )

            # Wait for all paragraph translations to complete
            await asyncio.gather(*paragraph_tasks)

            # Translate tables concurrently
            table_tasks = []
            for table in doc.tables:
                table_tasks.append(translate_table_async(
                    table, src_lang, tgt_lang))

            # Wait for all table translations to complete
            await asyncio.gather(*table_tasks)

            # Save translated document
            output_docx_path = os.path.join(
                temp_dir, f"{input_file_name}_{src_lang}_to_{tgt_lang}.docx")
            doc.save(output_docx_path)

            # Read the file into memory
            async with aiofiles.open(output_docx_path, 'rb') as file:
                file_content = await file.read()
                translated_files[tgt_lang] = {
                    'filename': f"{input_file_name}_{src_lang}_to_{tgt_lang}.docx",
                    'content': file_content
                }

            # Record end time
            stats["languages"][tgt_lang]["end_time"] = datetime.datetime.now()
            time_diff = stats["languages"][tgt_lang]["end_time"] - \
                stats["languages"][tgt_lang]["start_time"]
            hours, remainder = divmod(time_diff.seconds, 3600)
            minutes, seconds = divmod(remainder, 60)

            logger.info(
                f"Translation completed for {tgt_lang} in {hours}h {minutes}m {seconds}s")

        except Exception as e:
            logger.error(f"Error translating to {tgt_lang}: {e}")
            raise

    # Process all target languages concurrently
    language_tasks = [process_language(tgt_lang) for tgt_lang in tgt_langs]
    await asyncio.gather(*language_tasks)

    # Write translation statistics to log file
    try:
        log_file_name = f"translation_stats_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        log_file_path = os.path.join(log_dir, log_file_name)

        async with aiofiles.open(log_file_path, "w") as log_file:
            await log_file.write(f"Document: {input_file_name}\n")
            await log_file.write(f"Source language: {src_lang}\n")
            await log_file.write(f"Total paragraphs: {stats['total_paragraphs']}\n")
            await log_file.write(f"Total tables: {stats['total_tables']}\n")
            await log_file.write("\nTranslation times:\n")

            for lang, timing in stats["languages"].items():
                if "end_time" in timing:
                    time_diff = timing["end_time"] - timing["start_time"]
                    hours, remainder = divmod(time_diff.seconds, 3600)
                    minutes, seconds = divmod(remainder, 60)
                    await log_file.write(f"{lang}: {hours}h {minutes}m {seconds}s\n")
                else:
                    await log_file.write(f"{lang}: FAILED\n")

    except Exception as e:
        logger.error(f"Error writing statistics log: {e}")

    return translated_files


@app.post("/translate-document")
async def translate_document(
    file: UploadFile = File(...),
    src_lang: str = Form(...),
    tgt_langs: str = Form(...),
    background_tasks: BackgroundTasks = None
):
    """Translate a document from source language to multiple target languages asynchronously."""
    try:
        # Parse target languages
        target_languages = [lang.strip() for lang in tgt_langs.split(",")]

        # Map language codes
        src_lang_code = language_mapping.get(src_lang, src_lang)
        tgt_lang_codes = [language_mapping.get(
            lang, lang) for lang in target_languages]

        # Validate input file
        if not file or not file.filename:
            raise HTTPException(status_code=400, detail="No file provided")

        # Create temporary directory
        with tempfile.TemporaryDirectory() as temp_dir:
            # Save uploaded file
            file_path = os.path.join(temp_dir, file.filename)
            async with aiofiles.open(file_path, "wb") as f:
                content = await file.read()
                await f.write(content)

            # Check file size
            file_size = os.path.getsize(file_path)
            if file_size > 50 * 1024 * 1024:  # 50 MB limit
                raise HTTPException(
                    status_code=400,
                    detail="File too large. Maximum file size is 50 MB."
                )

            # Get file extension
            file_extension = os.path.splitext(file.filename)[1].lower()
            file_name_without_ext = os.path.splitext(file.filename)[0]

            # Process based on file type
            if file_extension == ".pdf":
                # Convert PDF to DOCX
                docx_path = os.path.join(
                    temp_dir, f"{file_name_without_ext}.docx")
                try:
                    # Run PDF conversion in a thread pool
                    loop = asyncio.get_event_loop()
                    with ThreadPoolExecutor() as pool:
                        await loop.run_in_executor(
                            pool,
                            lambda: convert_pdf_to_docx(file_path, docx_path)
                        )
                    input_path = docx_path
                except Exception as e:
                    logger.error(f"PDF conversion error: {e}")
                    raise HTTPException(
                        status_code=400,
                        detail=f"Failed to convert PDF: {str(e)}"
                    )
            elif file_extension == ".docx":
                input_path = file_path
            else:
                raise HTTPException(
                    status_code=400,
                    detail=f"Unsupported file format: {file_extension}. Only PDF and DOCX are supported."
                )

            # Translate document
            try:
                logger.info(
                    f"Starting translation from {src_lang} to {', '.join(target_languages)}")
                translated_files = await translate_docx_async(
                    input_path,
                    src_lang_code,
                    tgt_lang_codes,
                    temp_dir,
                    file_name_without_ext
                )
            except Exception as e:
                logger.error(f"Translation error: {e}")
                raise HTTPException(
                    status_code=500,
                    detail=f"Error during translation: {str(e)}"
                )

            # Handle single language case
            if len(tgt_lang_codes) == 1:
                tgt_lang = tgt_lang_codes[0]
                if tgt_lang not in translated_files:
                    raise HTTPException(
                        status_code=500,
                        detail=f"Translation to {target_languages[0]} failed"
                    )

                file_info = translated_files[tgt_lang]
                file_content = file_info['content']
                file_name = file_info['filename']

                # Save to local storage
                output_dir = os.path.join("translated")
                os.makedirs(output_dir, exist_ok=True)
                output_path = os.path.join(output_dir, file_name)

                async with aiofiles.open(output_path, "wb") as f:
                    await f.write(file_content)

                # Return a redirect to the local download endpoint
                return RedirectResponse(
                    url=f"/download-translation/{file_name}",
                    status_code=303
                )

            # For multiple languages, save each file and generate download links
            download_links = []
            for i, tgt_lang in enumerate(tgt_lang_codes):
                if tgt_lang not in translated_files:
                    logger.warning(
                        f"Translation to {target_languages[i]} failed")
                    continue

                file_info = translated_files[tgt_lang]
                file_content = file_info['content']
                file_name = file_info['filename']

                # Save to local storage
                output_dir = os.path.join("translated")
                os.makedirs(output_dir, exist_ok=True)
                output_path = os.path.join(output_dir, file_name)

                async with aiofiles.open(output_path, "wb") as f:
                    await f.write(file_content)

                # Generate local download URL
                download_url = f"/download-translation/{file_name}"

                # Add user-friendly language name
                display_lang = reverse_language_mapping.get(tgt_lang, tgt_lang)

                download_links.append({
                    "language": display_lang,
                    "filename": file_name,
                    "download_url": download_url
                })

            # Return JSON response with download links
            return JSONResponse({
                "message": f"Translation completed to {len(download_links)} language(s).",
                "translated_files": download_links
            })

    except Exception as e:
        logger.error(f"Document translation error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/download-translation/{filename}")
async def download_translation(filename: str):
    """Download a translated file."""
    try:
        file_path = os.path.join("translated", filename)
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="File not found")

        return FileResponse(
            file_path,
            media_type='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            filename=filename
        )
    except Exception as e:
        logger.error(f"Error downloading file: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    # Using different port than api.py
    uvicorn.run(app, host="0.0.0.0", port=8001)
